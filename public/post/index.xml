<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on CVXR</title>
    <link>/post/</link>
    <description>Recent content in Posts on CVXR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Integer Programming</title>
      <link>/post/examples/cvxr_integer-programming/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_integer-programming/</guid>
      <description>Consider the following optimization problem.
\[ \begin{array}{ll} \mbox{Maximize} &amp;amp; x_1 + 2x_2 - 0.1x_3 - 3x_4\\ \mbox{subject to} &amp;amp; x_1, x_2, x_3, x_4 &amp;gt;= 0\\ &amp;amp; x_1 + x_2 &amp;lt;= 5\\ &amp;amp; 2x_1 - x_2 &amp;gt;= 0\\ &amp;amp; -x_1 + 3x_2 &amp;gt;= 0\\ &amp;amp; x_3 + x_4 &amp;gt;= 0.5\\ &amp;amp; x_3 &amp;gt;= 1.1\\ &amp;amp; x_3 \mbox{ is integer.} \end{array} \]
CVXR provides the Int and Bool constructors for specifying integer and boolean variables.</description>
    </item>
    
    <item>
      <title>Solver Peculiarities</title>
      <link>/post/examples/cvxr_solver-peculiarities/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_solver-peculiarities/</guid>
      <description>The default solvers in CVXR is ECOS. However, it is not always the best solver to use. As an example, let us consider again the catenary problem.
We will change the problem slightly to use a finer discretization from 101 points to say 501.
## Problem data m &amp;lt;- 501 L &amp;lt;- 2 h &amp;lt;- L / (m - 1) ## Form objective x &amp;lt;- Variable(m) y &amp;lt;- Variable(m) objective &amp;lt;- Minimize(sum(y)) ## Form constraints constraints &amp;lt;- list(x[1] == 0, y[1] == 1, x[m] == 1, y[m] == 1, diff(x)^2 + diff(y)^2 &amp;lt;= h^2) ## Solve the catenary problem prob &amp;lt;- Problem(objective, constraints) result &amp;lt;- solve(prob) The solution status is no longer optimal.</description>
    </item>
    
    <item>
      <title>Using Other Solvers</title>
      <link>/post/examples/cvxr_using-other-solvers/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_using-other-solvers/</guid>
      <description>Introduction The default installation of CVXR comes with two (imported) open source solvers:
 ECOS and its mixed integer cousin ECOS_BB via the CRAN package ECOSolveR SCS via the CRAB package scs.  CVXR can also make use of several other open source solvers implemented in R packages.
 The linear and mixed integer programming package lpSolve via the lpSolveAPI package The linear and mixed integer programming package GLPK via the Rglpk package.</description>
    </item>
    
    <item>
      <title>Censored Regression</title>
      <link>/post/examples/cvxr_censored-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_censored-regression/</guid>
      <description>Introduction Data collected from an experimental study is sometimes censored, so that only partial information is known about a subset of observations. For instance, when measuring the lifespan of mice, we may find a number of subjects live beyond the duration of the project. Thus, all we know is the lower bound on their lifespan. This right censoring can be incorporated into a regression model via convex optimization.
Suppose that only \(K\) of our observations \((x_i,y_i)\) are fully observed, and the remaining are censored such that we observe \(x_i\), but only know \(y_i \geq D\) for \(i = K+1,\ldots,m\) and some constant \(D \in {\mathbf R}\).</description>
    </item>
    
    <item>
      <title>Elastic Net</title>
      <link>/post/examples/cvxr_elastic-net/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_elastic-net/</guid>
      <description>Introduction Often in applications, we encounter problems that require regularization to prevent overfitting, introduce sparsity, facilitate variable selection, or impose prior distributions on parameters. Two of the most common regularization functions are the \(l_1\)-norm and squared \(l_2\)-norm, combined in the elastic net regression model (H. Zou 2005, Friedman, Hastie, and Tibshirani (2010)),
\[ \begin{array}{ll} \underset{\beta}{\mbox{minimize}} &amp;amp; \frac{1}{2m}\|y - X\beta\|_2^2 + \lambda(\frac{1-\alpha}{2}\|\beta\|_2^2 + \alpha\|\beta\|_1). \end{array} \]
Here \(\lambda \geq 0\) is the overall regularization weight and \(\alpha \in [0,1]\) controls the relative \(l_1\) versus squared \(l_2\) penalty.</description>
    </item>
    
    <item>
      <title>Fastest Mixing Markov Chain</title>
      <link>/post/examples/cvxr_fast-mixing-mc/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_fast-mixing-mc/</guid>
      <description>Introduction This example is derived from the results in Boyd, Diaconis, and Xiao (2004), section 2. Let \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\) be a connected graph with vertices \(\mathcal{V} = \{1,\ldots,n\}\) and edges \(\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}\). Assume that \((i,i) \in \mathcal{E}\) for all \(i = 1,\ldots,n\), and \((i,j) \in \mathcal{E}\) implies \((j,i) \in \mathcal{E}\). Under these conditions, a discrete-time Markov chain on \(\mathcal{V}\) will have the uniform distribution as one of its equilibrium distributions.</description>
    </item>
    
    <item>
      <title>Getting Faster Results</title>
      <link>/post/examples/cvxr_speed/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_speed/</guid>
      <description>Warning The solution described below is useful when you mathematically know a problem is DCP-compliant and none of your data inputs will change the nature of the problem. We recommend that users check the DCP-compliance of a problem (via a call to is_dcp(prob) for example) at least once to ensure this is the case. Not verifying DCP-compliance may result in garbage!
 Introduction As was remarked in the introduction to CVXR, its chief advantage is flexibility: you can specify a problem in close to mathematical form and CVXR solves it for you, if it can.</description>
    </item>
    
    <item>
      <title>Huber Regression</title>
      <link>/post/examples/cvxr_huber-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_huber-regression/</guid>
      <description>Introduction Huber regression (Huber 1964) is a regression technique that is robust to outliers. The idea is to use a different loss function rather than the traditional least-squares; we solve
\[\begin{array}{ll} \underset{\beta}{\mbox{minimize}} &amp;amp; \sum_{i=1}^m \phi(y_i - x_i^T\beta) \end{array}\] for variable \(\beta \in {\mathbf R}^n\), where the loss \(\phi\) is the Huber function with threshold \(M &amp;gt; 0\), \[ \phi(u) = \begin{cases} u^2 &amp;amp; \mbox{if } |u| \leq M \\ 2Mu - M^2 &amp;amp; \mbox{if } |u| &amp;gt; M.</description>
    </item>
    
    <item>
      <title>Kelly Gambling</title>
      <link>/post/examples/cvxr_kelly-strategy/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_kelly-strategy/</guid>
      <description>Introduction In Kelly gambling (Kelly 1956), we are given the opportunity to bet on \(n\) possible outcomes, which yield a random non-negative return of \(r \in {\mathbf R}_+^n\). The return \(r\) takes on exactly \(K\) values \(r_1,\ldots,r_K\) with known probabilities \(\pi_1,\ldots,\pi_K\). This gamble is repeated over \(T\) periods. In a given period \(t\), let \(b_i \geq 0\) denote the fraction of our wealth bet on outcome \(i\). Assuming the \(n\)th outcome is equivalent to not wagering (it returns one with certainty), the fractions must satisfy \(\sum_{i=1}^n b_i = 1\).</description>
    </item>
    
    <item>
      <title>L1 Trend Filtering</title>
      <link>/post/examples/cvxr_l1-trend-filtering/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_l1-trend-filtering/</guid>
      <description>Introduction Kim et al. (2009) propose the \(l_1\) trend filtering method for trend estimation. The method solves an optimization problem of the form
\[ \begin{array}{ll} \underset{\beta}{\mbox{minimize}} &amp;amp; \frac{1}{2}\sum_{i=1}^m (y_i - \beta_i)^2 + \lambda ||D\beta||_1 \end{array} \] where the variable to be estimated is \(\beta\) and we are given the problem data \(y\) and \(\lambda\). The matrix \(D\) is the second-order difference matrix,
\[ D = \left[ \begin{matrix} 1 &amp;amp; -2 &amp;amp; 1 &amp;amp; &amp;amp; &amp;amp; &amp;amp; \\ &amp;amp; 1 &amp;amp; -2 &amp;amp; 1 &amp;amp; &amp;amp; &amp;amp; \\ &amp;amp; &amp;amp; \ddots &amp;amp; \ddots &amp;amp; \ddots &amp;amp; &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; 1 &amp;amp; -2 &amp;amp; 1 &amp;amp; \\ &amp;amp; &amp;amp; &amp;amp; &amp;amp; 1 &amp;amp; -2 &amp;amp; 1\\ \end{matrix} \right].</description>
    </item>
    
    <item>
      <title>Largest Ball in a Polyhedron in 2D</title>
      <link>/post/examples/cvxr_2d_ball/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_2d_ball/</guid>
      <description>Problem The following is a problem from Boyd and Vandenberghe (2004), section 4.3.1.
Find the largest Euclidean ball (i.e.Â its center and radius) that lies in a polyhedron described by affine inequalites:
\[ P = {x : a_i&amp;#39;*x &amp;lt;= b_i, i=1,...,m} \]
where x is in \({\mathbf R}^2\).
We define variables that determine the polyhedron.
a1 &amp;lt;- matrix(c(2,1)) a2 &amp;lt;- matrix(c(2,-1)) a3 &amp;lt;- matrix(c(-1,2)) a4 &amp;lt;- matrix(c(-1,-2)) b &amp;lt;- rep(1,4) Next, we formulate the CVXR problem.</description>
    </item>
    
    <item>
      <title>Log-Concave Distribution Estimation</title>
      <link>/post/examples/cvxr_log-concave/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_log-concave/</guid>
      <description>Introduction Let \(n = 1\) and suppose \(x_i\) are i.i.d. samples from a log-concave discrete distribution on \(\{0,\ldots,K\}\) for some \(K \in {\mathbf Z}_+\). Define \(p_k := {\mathbf {Prob}}(X = k)\) to be the probability mass function. One method for estimating \(\{p_0,\ldots,p_K\}\) is to maximize the log-likelihood function subject to a log-concavity constraint , i.e.,
\[ \begin{array}{ll} \underset{p}{\mbox{maximize}} &amp;amp; \sum_{k=0}^K M_k\log p_k \\ \mbox{subject to} &amp;amp; p \geq 0, \quad \sum_{k=0}^K p_k = 1, \\ &amp;amp; p_k \geq \sqrt{p_{k-1}p_{k+1}}, \quad k = 1,\ldots,K-1, \end{array} \]</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/post/examples/cvxr_logistic-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_logistic-regression/</guid>
      <description>Introduction In classification problems, the goal is to predict the class membership based on predictors. Often there are two classes and one of the most popular methods for binary classification is logistic regression (Cox 1958, Freedman (2009)).
Suppose now that \(y_i \in \{0,1\}\) is a binary class indicator. The conditional response is modeled as \(y|x \sim \mbox{Bernoulli}(g_{\beta}(x))\), where \(g_{\beta}(x) = \frac{1}{1 + e^{-x^T\beta}}\) is the logistic function, and maximize the log-likelihood function, yielding the optimization problem</description>
    </item>
    
    <item>
      <title>Near Isotonic and Near Convex Regression</title>
      <link>/post/examples/cvxr_near-isotonic-and-near-convex-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_near-isotonic-and-near-convex-regression/</guid>
      <description>Given a set of data points \(y \in {\mathbf R}^m\), R. J. Tibshirani, Hoefling, and Tibshirani (2011) fit a nearly-isotonic approximation \(\beta \in {\mathbf R}^m\) by solving
\[ \begin{array}{ll} \underset{\beta}{\mbox{minimize}} &amp;amp; \frac{1}{2}\sum_{i=1}^m (y_i - \beta_i)^2 + \lambda \sum_{i=1}^{m-1}(\beta_i - \beta_{i+1})_+, \end{array} \]
where \(\lambda \geq 0\) is a penalty parameter and \(x_+ =\max(x,0)\). This can be directly formulated in CVXR. As an example, we use global warming data from the Carbon Dioxide Information Analysis Center (CDIAC).</description>
    </item>
    
    <item>
      <title>Portfolio Optimization</title>
      <link>/post/examples/cvxr_portfolio-optimization/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_portfolio-optimization/</guid>
      <description>Introduction In this example, we solve the Markowitz portfolio problem under various constraints (Markowitz 1952; Roy 1952; Lobo, Fazel, and Boyd 2007).
We have \(n\) assets or stocks in our portfolio and must determine the amount of money to invest in each. Let \(w_i\) denote the fraction of our budget invested in asset \(i = 1,\ldots,m\), and let \(r_i\) be the returns (, fractional change in price) over the period of interest.</description>
    </item>
    
    <item>
      <title>Quantile Regression</title>
      <link>/post/examples/cvxr_quantile-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_quantile-regression/</guid>
      <description>Introduction Quantile regression is another variation on least squares . The loss is the tilted \(l_1\) function,
\[ \phi(u) = \tau\max(u,0) - (1-\tau)\max(-u,0) = \frac{1}{2}|u| + \left(\tau - \frac{1}{2}\right)u, \]
where \(\tau \in (0,1)\) specifies the quantile. The problem as before is to minimize the total residual loss. This model is commonly used in ecology, healthcare, and other fields where the mean alone is not enough to capture complex relationships between variables.</description>
    </item>
    
    <item>
      <title>Saturating Hinges Fit</title>
      <link>/post/examples/cvxr_saturating_hinges/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_saturating_hinges/</guid>
      <description>Introduction The following example comes from work on saturating splines in N. Boyd et al. (2016). Adaptive regression splines are commonly used in statistical modeling, but the instability they exhibit beyond their boundary knots makes extrapolation dangerous. One way to correct this issue for linear splines is to require they saturate: remain constant outside their boundary. This problem can be solved using a heuristic that is an extension of lasso regression, producing a weighted sum of hinge functions, which we call a saturating hinge.</description>
    </item>
    
    <item>
      <title>Sparse Inverse Covariance Estimation</title>
      <link>/post/examples/cvxr_sparse_inverse_covariance_estimation/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_sparse_inverse_covariance_estimation/</guid>
      <description>Introduction Assume we are given i.i.d. observations \(x_i \sim N(0,\Sigma)\) for \(i = 1,\ldots,m\), and the covariance matrix \(\Sigma \in {\mathbf S}_+^n\), the set of symmetric positive semidefinite matrices, has a sparse inverse \(S = \Sigma^{-1}\). Let \(Q = \frac{1}{m-1}\sum_{i=1}^m (x_i - \bar x)(x_i - \bar x)^T\) be our sample covariance. One way to estimate \(\Sigma\) is to maximize the log-likelihood with the prior knowledge that \(S\) is sparse (Friedman, Hastie, and Tibshirani 2008), which amounts to the optimization problem:</description>
    </item>
    
    <item>
      <title>The Catenary Problem</title>
      <link>/post/examples/cvxr_catenary/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_catenary/</guid>
      <description>Introduction A chain with uniformly distributed mass hangs from the endpoints \((0,1)\) and \((1,1)\) on a 2-D plane. Gravitational force acts in the negative \(y\) direction. Our goal is to find the shape of the chain in equilibrium, which is equivalent to determining the \((x,y)\) coordinates of every point along its curve when its potential energy is minimized.
This is the famous catenary problem.
 A Discrete Version To formulate as an optimization problem, we parameterize the chain by its arc length and divide it into \(m\) discrete links.</description>
    </item>
    
    <item>
      <title>Tutorial Examples</title>
      <link>/post/cvxr_examples/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_examples/</guid>
      <description>Many of the examples here were ported from the CVXPY site, although we have added some new ones as well.
Basic Examples  Largest Euclidean ball in a 2D polyhedron Catenary Problem Integer Programming   Regression Examples  Huber Regression Logistic Regression Quantile Regression Censored Regression Isotonic Regression   Penalized Models  Near Isotonic and Near Convex Regression \(L_1\) Trend Filtering Elastic Net Saturating Hinges   Miscellaneous  Direct Standardization Log-Concave Density Estimation Sparse Inverse Covariance Estimation Kelly Gambling Fastest Mixing Markov Chain Portfolio Optimization   Advanced Topics  Getting Faster Results Using Other Solvers Solver Peculiarities   References  </description>
    </item>
    
    <item>
      <title>Direct Standardization</title>
      <link>/post/examples/cvxr_direct-standardization/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_direct-standardization/</guid>
      <description>Introduction Consider a set of observations \((x_i,y_i)\) drawn non-uniformly from an unknown distribution. We know the expected value of the columns of \(X\), denoted by \(b \in {\mathbf R}^n\), and want to estimate the true distribution of \(y\). This situation may arise, for instance, if we wish to analyze the health of a population based on a sample skewed toward young males, knowing the average population-level sex, age, etc. The empirical distribution that places equal probability \(1/m\) on each \(y_i\) is not a good estimate.</description>
    </item>
    
    <item>
      <title>Isotonic Regression</title>
      <link>/post/examples/cvxr_isotonic-regression/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_isotonic-regression/</guid>
      <description>Introduction Isotonic regression is regression with monotonic constraints. There are several packages in R to fit isotonic regression models. In this example, we consider isotone which uses a pooled-adjacent-violators algorithm (PAVA) and active set methods to perform the fit.
 Pituitary Data Example We will use data from the isotone package (see de Leeuw, Hornik, and Mair (2009)) on the size of pituitary fissures for 11 subjects between 8 and 14 years of age.</description>
    </item>
    
    <item>
      <title>CVXR Functions</title>
      <link>/post/cvxr_functions/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_functions/</guid>
      <description>Functions Here we describe the functions that can be applied to CVXR expressions. CVXR uses the function information in this section and the Disciplined Convex Programming tools to mark expressions with a sign and curvature.
Operators The infix operators +, -, *, %*%, / are treated as functions. + and - are affine functions. * and / are affine in CVXR because expr1*expr2 and expr1 %*% expr2 are allowed only when one of the expressions is constant and expr1/expr2 is allowed only when expr2 is a scalar constant.</description>
    </item>
    
    <item>
      <title>Convex Optimization in R</title>
      <link>/post/cvxr/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr/</guid>
      <description>What is CVXR? CVXR is an R package that provides an object-oriented modeling language for convex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows the user to formulate convex optimization problems in a natural mathematical syntax rather than the restrictive standard form required by most solvers. The user specifies an objective and set of constraints by combining constants, variables, and parameters using a library of functions with known mathematical properties.</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to `CVXR`</title>
      <link>/post/examples/cvxr_gentle-intro/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_gentle-intro/</guid>
      <description>Introduction Welcome to CVXR: a modeling language for describing and solving convex optimization problems that follows the natural, mathematical notation of convex optimization rather than the requirements of any particular solver. The purpose of this document is both to introduce the reader to CVXR and to generate excitement for its possibilities in the field of statistics.
Convex optimization is a powerful and very general tool. As a practical matter, the set of convex optimization problems includes almost every optimization problem that can be solved exactly and efficiently (i.</description>
    </item>
    
    <item>
      <title>Discplined Convex Programming</title>
      <link>/post/cvxr_dcp/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_dcp/</guid>
      <description>Disciplined convex programming (DCP) is a system for constructing mathematical expressions with known curvature from a given library of base functions. CVXR uses DCP to ensure that the specified optimization problems are convex.
This section of the tutorial explains the rules of DCP and how they are applied by CVXR.
Visit dcp.stanford.edu for a more interactive introduction to DCP.
Expressions Expressions in CVXR are formed from variables, numerical constants such as R vectors and matrices, the standard arithmetic operators +, -, *, %*%, /, and a library of functions.</description>
    </item>
    
    <item>
      <title>Frequently Asked Questions</title>
      <link>/post/cvxr_faq/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_faq/</guid>
      <description>Questions Where can I go for help? How do I reformulate my problem to be DCP compliant? I want to report a bug.   Answers Where can I go for help?  Please post questions to the cvx tag on StackOverflow.
My problem is not DCP compliant. How do I reformulate it?  It is impossible for us to analyze every individual problem and determine whether it can be reformulated as a DCP problem.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/post/examples/cvxr_intro/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_intro/</guid>
      <description>Consider a simple linear regression problem where it is desired to estimate a set of parameters using a least squares criterion.
We generate some synthetic data where we know the model completely, that is
\[ Y = X\beta + \epsilon \]
where \(Y\) is a \(100\times 1\) vector, \(X\) is a \(100\times 10\) matrix, \(\beta = [-4,\ldots ,-1, 0, 1, \ldots, 5]\) is a \(10\times 1\) vector, and \(\epsilon \sim N(0, 1)\).</description>
    </item>
    
  </channel>
</rss>