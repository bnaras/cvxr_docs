<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head lang="en-us">
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
	<meta name="description" content="Disciplined Convex Programming in R">
	<meta name="generator" content="Hugo 0.30.2" />
	
	<title>Saturating Hinges Fit &mdash; CVXR</title>
	
	<link rel="stylesheet" href="../../../css/alabaster.css" type="text/css" />
	<link rel="stylesheet" href="../../../css/pygments.css" type="text/css" />

	

	<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon"/>
</head>

	<body role="document">
		<div class="document">
			<div class="documentwrapper">
				<div class="bodywrapper">
					<div class="body" role="main">
						
	<h1>Saturating Hinges Fit</h1>
	
	<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The following example comes from work on saturating splines in <span class="citation">N. Boyd et al. (<a href="#ref-BoydHastie:2016">2016</a>)</span>. Adaptive regression splines are commonly used in statistical modeling, but the instability they exhibit beyond their boundary knots makes extrapolation dangerous. One way to correct this issue for linear splines is to require they <em>saturate</em>: remain constant outside their boundary. This problem can be solved using a heuristic that is an extension of lasso regression, producing a weighted sum of hinge functions, which we call a <em>saturating hinge</em>.</p>
<p>For simplicity, consider the univariate case with <span class="math inline">\(n = 1\)</span>. Assume we are given knots <span class="math inline">\(t_1 &lt; t_2 &lt; \cdots &lt; t_k\)</span> where each <span class="math inline">\(t_j \in {\mathbf R}\)</span>. Let <span class="math inline">\(h_j\)</span> be a hinge function at knot <span class="math inline">\(t_j\)</span>, , <span class="math inline">\(h_j(x) = \max(x-t_j,0)\)</span>, and define <span class="math inline">\(f(x) = w_0 + \sum_{j=1}^k w_jh_j(x)\)</span>. We want to solve</p>
<p><span class="math display">\[
\begin{array}{ll} 
\underset{w_0,w}{\mbox{minimize}} &amp; \sum_{i=1}^m \ell(y_i, f(x_i)) + \lambda\|w\|_1 \\
\mbox{subject to} &amp; \sum_{j=1}^k w_j = 0
\end{array}
\]</span></p>
<p>for variables <span class="math inline">\((w_0,w) \in {\mathbf R} \times {\mathbf R}^k\)</span>. The function <span class="math inline">\(\ell:{\mathbf R} \times {\mathbf R} \rightarrow {\mathbf R}\)</span> is the loss associated with every observation, and <span class="math inline">\(\lambda \geq 0\)</span> is the penalty weight. In choosing our knots, we set <span class="math inline">\(t_1 = \min(x_i)\)</span> and <span class="math inline">\(t_k = \max(x_i)\)</span> so that by construction, the estimate <span class="math inline">\(\hat f\)</span> will be constant outside <span class="math inline">\([t_1,t_k]\)</span>.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>We demonstrate this technique on the bone density data for female patients from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL">2001</a>)</span>, section 5.4. There are a total of <span class="math inline">\(m = 259\)</span> observations. Our response <span class="math inline">\(y_i\)</span> is the change in spinal bone density between two visits, and our predictor <span class="math inline">\(x_i\)</span> is the patient’s age. We select <span class="math inline">\(k = 10\)</span> knots about evenly spaced across the range of <span class="math inline">\(X\)</span> and fit a saturating hinge with squared error loss <span class="math inline">\(\ell(y_i, f(x_i)) = (y_i - f(x_i))^2\)</span>.</p>
<pre class="r"><code>suppressMessages(suppressWarnings(library(CVXR)))
## Import and sort data
data(bone, package = &quot;ElemStatLearn&quot;)
X &lt;- bone[bone$gender == &quot;female&quot;,]$age
y &lt;- bone[bone$gender == &quot;female&quot;,]$spnbmd
ord &lt;- order(X, decreasing = FALSE)
X &lt;- X[ord]
y &lt;- y[ord]

## Choose knots evenly distributed along domain
k &lt;- 10
lambdas &lt;- c(1, 0.5, 0.01)
idx &lt;- floor(seq(1, length(X), length.out = k))
knots &lt;- X[idx]</code></pre>
<p>In <code>R</code>, we first define the estimation and loss functions:</p>
<pre class="r"><code>## Saturating hinge
f_est &lt;- function(x, knots, w0, w) {
    hinges &lt;- sapply(knots, function(t) { pmax(x - t, 0) })
    w0 + hinges %*% w
}

## Loss function
loss_obs &lt;- function(y, f) { (y - f)^2 }</code></pre>
<p>This allows us to easily test different losses and knot locations later. The rest of the set-up is similar to previous examples. We assume that <code>knots</code> is an <code>R</code> vector representing <span class="math inline">\((t_1,\ldots,t_k)\)</span>.</p>
<pre class="r"><code>## Form problem
w0 &lt;- Variable(1)
w &lt;- Variable(k)
loss &lt;- sum(loss_obs(y, f_est(X, knots, w0, w)))
constr &lt;- list(sum(w) == 0)

xrange &lt;- seq(min(X), max(X), length.out = 100)
splines &lt;- matrix(0, nrow = length(xrange), ncol = length(lambdas))</code></pre>
<p>The optimal weights are retrieved using separate calls, as shown below.</p>
<pre class="r"><code>for (i in seq_along(lambdas)) {
    lambda &lt;- lambdas[i]
    reg &lt;- lambda * p_norm(w, 1)
    obj &lt;- loss + reg
    prob &lt;- Problem(Minimize(obj), constr)

    ## Solve problem and save spline weights
    result &lt;- solve(prob)
    w0s &lt;- result$getValue(w0)
    ws &lt;- result$getValue(w)
    splines[, i] &lt;- f_est(xrange, knots, w0s, ws)
}</code></pre>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>We plot the fitted saturating hinges in Figure 1 below. As expected, when <span class="math inline">\(\lambda\)</span> increases, the spline exhibits less variation and grows flatter outside its boundaries.</p>
<pre class="r"><code>d &lt;- data.frame(xrange, splines)
names(d) &lt;- c(&quot;x&quot;, paste0(&quot;lambda_&quot;, seq_len(length(lambdas))))
plot.data &lt;- gather(d, key = &quot;lambda&quot;, value = &quot;spline&quot;,
                    &quot;lambda_1&quot;, &quot;lambda_2&quot;, &quot;lambda_3&quot;, factor_key = TRUE)
ggplot() +
    geom_point(mapping = aes(x = X, y = y)) +
    geom_line(data = plot.data, mapping = aes(x = x, y = spline, color = lambda)) +
    scale_color_discrete(name = expression(lambda),
                         labels = sprintf(&quot;%0.2f&quot;, lambdas)) +
    labs(x = &quot;Age&quot;, y = &quot;Change in Bone Density&quot;) +
    theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="../../../post/examples/cvxr_saturating-hinges_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The squared error loss works well in this case, but the Huber loss is preferred when the dataset contains large outliers. To see this, we add 50 randomly generated outliers to the bone density data and re-estimate the saturating hinges.</p>
<pre class="r"><code>## Add outliers to data
set.seed(1)
nout &lt;- 50
X_out &lt;- runif(nout, min(X), max(X))
y_out &lt;- runif(nout, min(y), 3*max(y)) + 0.3
X_all &lt;- c(X, X_out)
y_all &lt;- c(y, y_out)

## Solve with squared error loss
loss_obs &lt;- function(y, f) { (y - f)^2 }
loss &lt;- sum(loss_obs(y_all, f_est(X_all, knots, w0, w)))
prob &lt;- Problem(Minimize(loss + reg), constr)
result &lt;- solve(prob)
spline_sq &lt;- f_est(xrange, knots, result$getValue(w0), result$getValue(w))

## Solve with Huber loss
loss_obs &lt;- function(y, f, M) { huber(y - f, M) }
loss &lt;- sum(loss_obs(y, f_est(X, knots, w0, w), 0.01))
prob &lt;- Problem(Minimize(loss + reg), constr)
result &lt;- solve(prob)
spline_hub &lt;- f_est(xrange, knots, result$getValue(w0), result$getValue(w))</code></pre>
<p>Figure 2 shows the results. For a Huber loss with <span class="math inline">\(M = 0.01\)</span>, the resulting spline is fairly smooth and follows the shape of the original data, as opposed to the spline using squared error loss, which is biased upwards by a significant amount.</p>
<pre class="r"><code>d &lt;- data.frame(xrange, spline_hub, spline_sq)
names(d) &lt;- c(&quot;x&quot;, &quot;Huber&quot;, &quot;Squared&quot;)
plot.data &lt;- gather(d, key = &quot;loss&quot;, value = &quot;spline&quot;,
                    &quot;Huber&quot;, &quot;Squared&quot;, factor_key = TRUE)
ggplot() +
    geom_point(mapping = aes(x = X, y = y)) +
    geom_point(mapping = aes(x = X_out, y = y_out), color = &quot;orange&quot;) +
    geom_line(data = plot.data, mapping = aes(x = x, y = spline, color = loss)) +
    scale_color_discrete(name = &quot;Loss&quot;,
                         labels = c(&quot;Huber&quot;, &quot;Squared&quot;)) +
    labs(x = &quot;Age&quot;, y = &quot;Change in Bone Density&quot;) +
    theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="../../../post/examples/cvxr_saturating-hinges_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] C
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
## [1] CVXR_0.99     ggplot2_2.2.1 tidyr_0.8.1  
## 
## loaded via a namespace (and not attached):
##  [1] gmp_0.5-13.1      Rcpp_0.12.17      pillar_1.2.2     
##  [4] compiler_3.5.0    plyr_1.8.4        R.methodsS3_1.7.1
##  [7] R.utils_2.6.0     tools_3.5.0       bit_1.1-13       
## [10] digest_0.6.15     evaluate_0.10.1   tibble_1.4.2     
## [13] gtable_0.2.0      lattice_0.20-35   rlang_0.2.0      
## [16] Matrix_1.2-14     yaml_2.1.19       blogdown_0.6.3   
## [19] xfun_0.1          Rmpfr_0.7-0       ECOSolveR_0.4    
## [22] stringr_1.3.1     knitr_1.20        tidyselect_0.2.4 
## [25] rprojroot_1.3-2   bit64_0.9-7       grid_3.5.0       
## [28] glue_1.2.0        R6_2.2.2          rmarkdown_1.9.14 
## [31] bookdown_0.7      purrr_0.2.4       magrittr_1.5     
## [34] scs_1.1-1         backports_1.1.2   scales_0.5.0     
## [37] htmltools_0.3.6   colorspace_1.3-2  labeling_0.3     
## [40] stringi_1.2.2     lazyeval_0.2.1    munsell_0.4.3    
## [43] R.oo_1.22.0</code></pre>
</div>
<div id="source" class="section level2">
<h2>Source</h2>
<p><a href="https://github.com/bnaras/cvxr_docs/blob/master/content/post/examples/cvxr_saturating-hinges.Rmd">R Markdown</a></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-BoydHastie:2016">
<p>Boyd, N., T. Hastie, S. Boyd, B. Recht, and M. Jordan. 2016. “Saturating Splines and Feature Selection.” <em>arXiv Preprint arXiv:1609.06764</em>.</p>
</div>
<div id="ref-ESL">
<p>Hastie, T., R. Tibshirani, and J. Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
</div>
</div>



						
					</div>
				</div>
			</div>
			
			<div class="sphinxsidebar" role="navigation" aria-label="main navigation">
	<div class="sphinxsidebarwrapper">
		<p class="logo">
			<a href="../../../">
				<img class="logo" src="../../../favicon.ico" alt="Logo"/>
				<h1 class="logo logo-name"></h1>
			</a>
		</p>
		
		<p class="blurb">Disciplined Convex Programming in R</p>

		

	<p>
		<iframe src="https://ghbtns.com/github-btn.html?user=bnaras&repo=cvxr_docs&type=watch&count=true&size=large"
		allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
	</p>

	

	
		

		

<h3>Navigation</h3>
<ul>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../index.html">Home</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="https://anqif.github.io/CVXR">Package Docs</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/examples/cvxr_intro/">A Quick Intro</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/examples/cvxr_gentle-intro/">A Longer Intro</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_examples/">Tutorial Examples</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_dcp/">DCP</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_faq/">FAQ</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_functions/">Function Reference</a>
	</li>
	
</ul>


		<h3>Related Topics</h3>
<ul>
  <li><a href="../../../">Documentation overview</a><ul>
  <li>Previous: <a href="../../../post/examples/cvxr_sparse_inverse_covariance_estimation/" title="Sparse Inverse Covariance Estimation">Sparse Inverse Covariance Estimation</a></li>
  <li>Next: <a href="../../../post/examples/cvxr_quantile-regression/" title="Quantile Regression">Quantile Regression</a></li>
</ul>

	</div>
</div>
<div class="clearer"></div>
</div>
			<script type="text/javascript" src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


			

			

			
		</div>
	</body>
</html>