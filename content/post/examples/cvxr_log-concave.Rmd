---
title: Log-Concave Distribution Estimation
author: Anqi Fu and Balasubramanian Narasimhan
date: '2017-11-02'
slug: cvxr_log-concave
bibliography: ../bibtex/cvxr_refs.bib
link-citations: true
categories: []
tags: []
---

## Introduction

Let $n = 1$ and suppose $x_i$ are i.i.d. samples from a log-concave
discrete distribution on $\{0,\ldots,K\}$ for some
$K \in \integers_+$. Define $p_k := \Prob(X = k)$ to be the
probability mass function. One method for estimating
$\{p_0,\ldots,p_K\}$ is to maximize the log-likelihood function
subject to a log-concavity constraint \citep{mlelogcave}, i.e.,

\begin{array}{ll}
\underset{p}{\mbox{maximize}} & \sum_{k=0}^K M_k\log p_k \\
\mbox{subject to} & p \geq 0, \quad \sum_{k=0}^K p_k = 1, \\
& p_k \geq \sqrt{p_{k-1}p_{k+1}}, \quad k = 1,\ldots,K-1,
\end{array}

where $p \in {\mathbf R}^{K+1}$ is our variable of interest and $M_k$
represents the number of observations equal to $k$, so that
$\sum_{k=0}^K M_k = m$. The problem as posed above is not
convex. However, we can transform it into a convex optimization
problem by defining new variables $u_k = \log p_k$ and relaxing the
equality constraint to $\sum_{k=0}^K p_k \leq 1$, since the latter
always holds tightly at an optimal solution. The result is

\begin{array}{ll}
\underset{u}{\mbox{maximize}} & \sum_{k=0}^K M_k u_k \\
\mbox{subject to} & \sum_{k=0}^K e^{u_k} \leq 1, \\
& u_k - u_{k-1} \geq u_{k+1} - u_k, \quad k = 1,\ldots,K-1.
\end{array}


## Example

We draw $m = 25$ observations from a log-concave distribution on
$\{0,\ldots,100\}$. We then estimate the probability mass function
using the above method and compare it with the empirical distribution.

```{r}
suppressMessages(suppressWarnings(library(CVXR)))
set.seed(1)
## Calculate a piecewise linear function
pwl_fun <- function(x, knots) {
    n <- nrow(knots)
    x0 <- sort(knots$x, decreasing = FALSE)
    y0 <- knots$y[order(knots$x, decreasing = FALSE)]
    slope <- diff(y0)/diff(x0)
    
    sapply(x, function(xs) {
        if(xs <= x0[1])
            y0[1] + slope[1]*(xs -x0[1])
        else if(xs >= x0[n])
            y0[n] + slope[n-1]*(xs - x0[n])
        else {
            idx <- which(xs <= x0)[1]
            y0[idx-1] + slope[idx-1]*(xs - x0[idx-1])
        }
    })
}
## Problem data
m <- 25
xrange <- 0:100
knots <- data.frame(x = c(0, 25, 65, 100), y = c(10, 30, 40, 15))
xprobs <- pwl_fun(xrange, knots)/15
xprobs <- exp(xprobs)/sum(exp(xprobs))
x <- sample(xrange, size = m, replace = TRUE, prob = xprobs)

K <- max(xrange)
xhist <- hist(x, breaks = -1:K, right = TRUE, include.lowest = FALSE)
counts <- xhist$counts
```

We now solve problem with log-concave constraint.

```{r}
u <- Variable(K+1)
obj <- t(counts) %*% u
constraints <- list(sum(exp(u)) <= 1, diff(u[1:K]) >= diff(u[2:(K+1)]))
prob <- Problem(Maximize(obj), constraints)
result <- solve(prob)
pmf <- result$getValue(exp(u))
```

The above lines transform the variables $u_k$ to $e^{u_k}$ before
calculating their resulting values. This is possible because `exp` is
a member of the `CVXR` library of atoms, so it can operate directly on
a `Variable` object such as `u`.

Below are the comparison plots of pmf and cdf. 

```{r}
library(ggplot2)
library(tidyr)
dens <- density(x, bw = "sj")
d <- data.frame(x = xrange, True = xprobs, Optimal = pmf,
                Empirical = approx(x = dens$x, y = dens$y, xout = xrange)$y)
plot.data <- gather(data = d, key = "Type", value = "Estimate", True, Empirical, Optimal,
                    factor_key = TRUE)
ggplot(plot.data) +
    geom_line(mapping = aes(x = x, y = Estimate, color = Type)) +
    theme(legend.position = "top")
```

```{r}
d <- data.frame(x = xrange, True = cumsum(xprobs),
                Empirical = cumsum(counts) / sum(counts),
                Optimal = cumsum(pmf))
plot.data <- gather(data = d, key = "Type", value = "Estimate", True, Empirical, Optimal,
                    factor_key = TRUE)
ggplot(plot.data) +
    geom_line(mapping = aes(x = x, y = Estimate, color = Type)) +
    theme(legend.position = "top")
```

From the figures we see that the estimated curve is much closer to the
true distribution, exhibiting a similar shape and number of peaks. In
contrast, the empirical probability mass function oscillates, failing
to be log-concave on parts of its domain. These differences are
reflected in the cumulative distribution functions as well.

## References
