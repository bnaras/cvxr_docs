---
title: Prototyping the pliable lasso
author: Anqi Fu and Balasubramanian Narasimhan
date: '2018-11-30'
slug: cvxr_pliable_lasso
bibliography: ../bibtex/cvxr_refs.bib
link-citations: true
categories: []
tags: []
params:
  mode: ignore
  testdata_dir: test_data
  data_dir: plasso
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><span class="citation">Tibshirani and Friedman (<a href="#ref-tibsjhf:2017" role="doc-biblioref">2017</a>)</span> propose a generalization of the lasso that allows the
model coefficients to vary as a function of a general set of modifying
variables, such as gender, age or time. The pliable lasso model has
the form</p>
<p><span class="math display">\[
\begin{equation}
\hat{y} = \beta_0{\mathbf 1} + Z\theta_0 + \sum_{j=1}^p(X_j\beta_j +
W_j\theta_j)
\end{equation}
\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> is the predicted <span class="math inline">\(N\times1\)</span> vector, <span class="math inline">\(\beta_0\)</span> is a
scalar, <span class="math inline">\(\theta_0\)</span> is a <span class="math inline">\(K\)</span>-vector, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <span class="math inline">\(N\times p\)</span> and
<span class="math inline">\(N\times K\)</span> matrices containing values of the predictor and modifying
variables respectively with <span class="math inline">\(W_j=X_j \circ Z\)</span> denoting the elementwise
multiplication of Z by column <span class="math inline">\(X_j\)</span> of <span class="math inline">\(X\)</span>.</p>
<p>The objective function used for pliable lasso is</p>
<p><span class="math display">\[
J(\beta_0, \theta_0, \beta, \Theta) = 
\frac{1}{2N}\sum_{i=1}^N (y_i-\hat{y}_i)^2 +
(1-\alpha)\lambda\sum_{j=1}^p\biggl(||(\beta_j,\theta_j)||_2 +
||\theta_j||_2\biggr) + \alpha\lambda\sum_{j,k}|\theta_{j,k}|_1.
\]</span></p>
<p>In the above, <span class="math inline">\(\Theta\)</span> is a <span class="math inline">\(p\times K\)</span> matrix of parameters with
<span class="math inline">\(j\)</span>-th row <span class="math inline">\(\theta_j\)</span> and individual entries <span class="math inline">\(\theta_{j,k}\)</span>, <span class="math inline">\(\lambda\)</span>
is a tuning parameters. As <span class="math inline">\(\alpha \rightarrow 1\)</span> (but <span class="math inline">\(&lt;1\)</span>), the
solution approaches the lasso solution. The default value used is
<span class="math inline">\(\alpha = 0.5.\)</span></p>
<p>An R package for the pliable lasso is forthcoming from the
authors. Nevertheless, the pliable lasso is an excellent example to
highlight the prototyping capabilities of <code>CVXR</code> in research. Along
the way, we also illustrate some additional atoms that are actually
needed in this example.</p>
</div>
<div id="the-pliable-lasso-in-cvxr" class="section level2">
<h2>The pliable lasso in <code>CVXR</code></h2>
<p>We will use a simulated example from section 3 of <span class="citation">Tibshirani and Friedman (<a href="#ref-tibsjhf:2017" role="doc-biblioref">2017</a>)</span> with
<span class="math inline">\(n=100\)</span>, <span class="math inline">\(p=50\)</span> and <span class="math inline">\(K=4\)</span>. The response is generated as</p>
<p><span class="math display">\[
\begin{eqnarray*}
y &amp;=&amp; \mu(x) + 0.5\cdot \epsilon;\ \ \epsilon \sim N(0, 1)\\
\mu(x) &amp;=&amp; x_1\beta_1 + x_2\beta_2 + x_3(\beta_3 e + 2z_1) +
x_4\beta_4(e - 2z_2);\ \ \beta = (2, -2, 2, 2, 0, 0, \ldots)
\end{eqnarray*}
\]</span></p>
<p>where <span class="math inline">\(e=(1,1,\ldots , 1)^T).\)</span></p>
<pre class="r"><code>## Simulation data.
set.seed(123)
N &lt;- 100
K &lt;- 4
p &lt;- 50
X &lt;- matrix(rnorm(n = N * p, mean = 0, sd = 1), nrow = N, ncol = p)
Z &lt;- matrix(rbinom(n = N * K, size = 1, prob = 0.5), nrow = N, ncol = K)

## Response model.
beta &lt;- rep(x = 0, times = p)
beta[1:4] &lt;- c(2, -2, 2, 2)
coeffs &lt;- cbind(beta[1], beta[2], beta[3] + 2 * Z[, 1], beta[4] * (1 - 2 * Z[, 2]))
mu &lt;- diag(X[, 1:4] %*% t(coeffs))
y &lt;- mu + 0.5 * rnorm(N, mean = 0, sd = 1)</code></pre>
<p>It seems worthwhile to write a function that will fit the model for us
so that we can customize a few things such as an intercept term,
verbosity etc. The function has the following structure with
comments as placeholders for code we shall construct later.</p>
<pre class="r"><code>plasso_fit &lt;- function(y, X, Z, lambda, alpha = 0.5, intercept = TRUE,
                       ZERO_THRESHOLD= 1e-6, verbose = FALSE) {
    N &lt;- length(y)
    p &lt;- ncol(X)
    K &lt;- ncol(Z)

    beta0 &lt;- 0
    if (intercept) {
        beta0 &lt;- Variable(1) * matrix(1, nrow = N, ncol = 1)
    }
    ## Define_Parameters
    ## Build_Penalty_Terms
    ## Compute_Fitted_Value
    ## Build_Objective
    ## Define_and_Solve_Problem
    ## Return_Values
}

## Fit pliable lasso using CVXR.
#pliable &lt;- pliable_lasso(y, X, Z, alpha = 0.5, lambda = lambda)</code></pre>
<div id="defining-the-parameters" class="section level3">
<h3>Defining the parameters</h3>
<p>The parameters are easy: we just have <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\theta_0\)</span> and
<span class="math inline">\(\Theta\)</span>.</p>
<pre class="r"><code>beta &lt;- Variable(p)
theta0 &lt;- Variable(K)
theta &lt;- Variable(p, K) ; theta_transpose &lt;- t(theta)</code></pre>
<p>Note that we also define the transpose of <span class="math inline">\(\Theta\)</span> for use later.</p>
</div>
<div id="the-penalty-terms" class="section level3">
<h3>The penalty terms</h3>
<p>There are three of them. The first term in the parenthesis,
<span class="math inline">\(\sum_{j=1}^p\biggl(||(\beta_j,\theta_j)||_2\biggr)\)</span>, involves components of
<span class="math inline">\(\beta\)</span> and rows of <span class="math inline">\(\Theta\)</span>. <code>CVXR</code> provides two functions to express
this norm:</p>
<ul>
<li><code>hstack</code> to bind columns of <span class="math inline">\(\beta\)</span> and the matrix <span class="math inline">\(\Theta\)</span>, the
equivalent of <code>rbind</code> in R,</li>
<li><code>cvxr_norm</code> which accepts a matrix variable and an <code>axis</code> denoting the axis along which
the norm is to be taken. The penalty requires us to use the row as
axis, so <code>axis = 1</code> per the usual R convention.</li>
</ul>
<p>The second term in the parenthesis <span class="math inline">\(\sum_{j}||\theta_j||_2\)</span> is also a norm
along rows as the <span class="math inline">\(\theta_j\)</span> are rows of <span class="math inline">\(\Theta\)</span>. And the last one is
simply a 1-norm.</p>
<pre class="r"><code>penalty_term1 &lt;- sum(cvxr_norm(hstack(beta, theta), 2, axis = 1))
penalty_term2 &lt;- sum(cvxr_norm(theta, 2, axis = 1))
penalty_term3 &lt;- sum(cvxr_norm(theta, 1))</code></pre>
</div>
<div id="the-fitted-value" class="section level3">
<h3>The fitted value</h3>
<p>Equation 1 above for <span class="math inline">\(\hat{y}\)</span> contains a sum:
<span class="math inline">\(\sum_{j=1}^p(X_j\beta_j + W_j\theta_j)\)</span>. This requires multiplication
of <span class="math inline">\(Z\)</span> by the columns of <span class="math inline">\(X\)</span> component-wise. That is a natural candidate
for a map-reduce combination: map the column multiplication function
appropriately and reduce using <code>+</code> to obtain the <code>XZ_term</code> below.</p>
<pre class="r"><code>xz_theta &lt;- lapply(seq_len(p),
                   function(j) (matrix(X[, j], nrow = N, ncol = K) * Z) %*% theta_transpose[, j])
XZ_term &lt;- Reduce(f = &#39;+&#39;, x = xz_theta)
y_hat &lt;- beta0 + X %*% beta + Z %*% theta0 + XZ_term</code></pre>
</div>
<div id="the-objective" class="section level3">
<h3>The objective</h3>
<p>The objective is now straightforward.</p>
<pre class="r"><code>objective &lt;- sum_squares(y - y_hat) / (2 * N) +
    (1 - alpha) * lambda * (penalty_term1 + penalty_term2) +
    alpha * lambda * penalty_term3</code></pre>
</div>
<div id="the-problem-and-its-solution" class="section level3">
<h3>The problem and its solution</h3>
<pre class="r"><code>prob &lt;- Problem(Minimize(objective))
result &lt;- solve(prob, verbose = TRUE)
beta_hat &lt;- result$getValue(beta)</code></pre>
</div>
<div id="the-return-values" class="section level3">
<h3>The return values</h3>
<p>We create a list with values of interest to us. However, since
sparsity is desired, we set values below <code>ZERO_THRESHOLD</code> to
zero.</p>
<pre class="r"><code>theta0_hat &lt;- result$getValue(theta0)
theta_hat &lt;- result$getValue(theta)

## Zero out stuff before returning
beta_hat[abs(beta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
theta0_hat[abs(theta0_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
theta_hat[abs(theta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
list(beta0_hat = if (intercept) result$getValue(beta0)[1] else 0.0,
     beta_hat = beta_hat,
     theta0_hat = theta0_hat,
     theta_hat = theta_hat,
     criterion = result$value)</code></pre>
</div>
</div>
<div id="the-full-function" class="section level2">
<h2>The full function</h2>
<p>We now put it all together.</p>
<pre class="r"><code>plasso_fit &lt;- function(y, X, Z, lambda, alpha = 0.5, intercept = TRUE,
                          ZERO_THRESHOLD= 1e-6, verbose = FALSE) {
    N &lt;- length(y)
    p &lt;- ncol(X)
    K &lt;- ncol(Z)

    beta0 &lt;- 0
    if (intercept) {
        beta0 &lt;- Variable(1) * matrix(1, nrow = N, ncol = 1)
    }
    beta &lt;- Variable(p)
    theta0 &lt;- Variable(K)
    theta &lt;- Variable(p, K) ; theta_transpose &lt;- t(theta)
    penalty_term1 &lt;- sum(cvxr_norm(hstack(beta, theta), 2, axis = 1))
    penalty_term2 &lt;- sum(cvxr_norm(theta, 2, axis = 1))
    penalty_term3 &lt;- sum(cvxr_norm(theta, 1))
    xz_theta &lt;- lapply(seq_len(p),
                       function(j) (matrix(X[, j], nrow = N, ncol = K) * Z) %*% theta_transpose[, j])
    XZ_term &lt;- Reduce(f = &#39;+&#39;, x = xz_theta)
    y_hat &lt;- beta0 + X %*% beta + Z %*% theta0 + XZ_term
    objective &lt;- sum_squares(y - y_hat) / (2 * N) +
        (1 - alpha) * lambda * (penalty_term1 + penalty_term2) +
        alpha * lambda * penalty_term3
    prob &lt;- Problem(Minimize(objective))
    result &lt;- solve(prob, verbose = TRUE)
    beta_hat &lt;- result$getValue(beta)
    theta0_hat &lt;- result$getValue(theta0)
    theta_hat &lt;- result$getValue(theta)
    
    ## Zero out stuff before returning
    beta_hat[abs(beta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
    theta0_hat[abs(theta0_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
    theta_hat[abs(theta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0
    list(beta0_hat = if (intercept) result$getValue(beta0)[1] else 0.0,
         beta_hat = beta_hat,
         theta0_hat = theta0_hat,
         theta_hat = theta_hat,
         criterion = result$value)
}</code></pre>
</div>
<div id="the-results" class="section level2">
<h2>The Results</h2>
<p>Using <span class="math inline">\(\lambda = 0.6\)</span> we fit the pliable lasso without an intercept</p>
<pre class="r"><code>result &lt;- plasso_fit(y, X, Z, lambda = 0.6, alpha = 0.5, intercept = FALSE)</code></pre>
<pre><code>## Acquiring MOSEK environment
## Problem
##   Name                   :                 
##   Objective sense        : min             
##   Type                   : CONIC (conic optimization problem)
##   Constraints            : 1056            
##   Cones                  : 101             
##   Scalar variables       : 1208            
##   Matrix variables       : 0               
##   Integer variables      : 0               
## 
## Optimizer started.
## Presolve started.
## Linear dependency checker started.
## Linear dependency checker terminated.
## Eliminator started.
## Freed constraints in eliminator : 0
## Eliminator terminated.
## Eliminator - tries                  : 1                 time                   : 0.00            
## Lin. dep.  - tries                  : 1                 time                   : 0.00            
## Lin. dep.  - number                 : 0               
## Presolve terminated. Time: 0.00    
## Problem
##   Name                   :                 
##   Objective sense        : min             
##   Type                   : CONIC (conic optimization problem)
##   Constraints            : 1056            
##   Cones                  : 101             
##   Scalar variables       : 1208            
##   Matrix variables       : 0               
##   Integer variables      : 0               
## 
## Optimizer  - threads                : 8               
## Optimizer  - solved problem         : the dual        
## Optimizer  - Constraints            : 456
## Optimizer  - Cones                  : 101
## Optimizer  - Scalar variables       : 1056              conic                  : 652             
## Optimizer  - Semi-definite variables: 0                 scalarized             : 0               
## Factor     - setup time             : 0.00              dense det. time        : 0.00            
## Factor     - ML order time          : 0.00              GP order time          : 0.00            
## Factor     - nonzeros before factor : 3.92e+04          after factor           : 4.56e+04        
## Factor     - dense dim.             : 6                 flops                  : 8.86e+06        
## ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  
## 0   2.4e+01  1.0e+00  3.2e+01  0.00e+00   3.000000000e+01   -1.000000000e+00  1.0e+00  0.01  
## 1   1.2e+01  5.0e-01  5.4e+00  8.77e-01   2.078449558e+01   9.709212181e+00   5.0e-01  0.01  
## 2   8.8e+00  3.6e-01  2.5e+00  2.61e+00   1.168682614e+01   6.734272672e+00   3.6e-01  0.02  
## 3   4.9e+00  2.0e-01  8.6e-01  2.07e+00   8.411428087e+00   6.525838553e+00   2.0e-01  0.02  
## 4   4.1e+00  1.7e-01  6.7e-01  1.53e+00   7.702191074e+00   6.148222756e+00   1.7e-01  0.02  
## 5   1.6e+00  6.5e-02  1.8e-01  1.58e+00   6.264895892e+00   5.725386628e+00   6.5e-02  0.02  
## 6   9.4e-01  3.9e-02  9.4e-02  8.10e-01   5.765244946e+00   5.383979330e+00   3.9e-02  0.03  
## 7   4.4e-01  1.8e-02  3.1e-02  9.72e-01   5.324228020e+00   5.138001741e+00   1.8e-02  0.03  
## 8   2.1e-01  8.7e-03  1.5e-02  3.50e-01   4.901890831e+00   4.766653954e+00   8.7e-03  0.03  
## 9   4.5e-02  1.9e-03  1.8e-03  8.29e-01   4.445497087e+00   4.410689416e+00   1.9e-03  0.04  
## 10  9.0e-03  3.7e-04  1.8e-04  7.41e-01   4.226277835e+00   4.217766978e+00   3.7e-04  0.04  
## 11  1.8e-03  7.4e-05  1.6e-05  8.61e-01   4.167494691e+00   4.165673328e+00   7.4e-05  0.04  
## 12  6.2e-04  2.5e-05  3.2e-06  9.55e-01   4.158096355e+00   4.157458056e+00   2.5e-05  0.04  
## 13  1.1e-04  4.6e-06  2.5e-07  9.81e-01   4.154142654e+00   4.154025382e+00   4.6e-06  0.05  
## 14  7.2e-06  2.1e-07  2.5e-09  9.95e-01   4.153324216e+00   4.153318763e+00   2.1e-07  0.05  
## 15  4.5e-04  1.3e-08  4.0e-11  1.00e+00   4.153291052e+00   4.153290713e+00   1.3e-08  0.05  
## 16  1.7e-02  3.4e-10  1.7e-13  1.00e+00   4.153288707e+00   4.153288698e+00   3.4e-10  0.06  
## Optimizer terminated. Time: 0.06    
## 
## 
## Interior-point solution summary
##   Problem status  : PRIMAL_AND_DUAL_FEASIBLE
##   Solution status : OPTIMAL
##   Primal.  obj: 4.1532887070e+00    nrm: 4e+02    Viol.  con: 3e-14    var: 0e+00    cones: 1e-09  
##   Dual.    obj: 4.1532886983e+00    nrm: 9e-01    Viol.  con: 9e-11    var: 8e-11    cones: 1e-10  
## Optimizer summary
##   Optimizer                 -                        time: 0.06    
##     Interior-point          - iterations : 16        time: 0.06    
##       Basis identification  -                        time: 0.00    
##         Primal              - iterations : 0         time: 0.00    
##         Dual                - iterations : 0         time: 0.00    
##         Clean primal        - iterations : 0         time: 0.00    
##         Clean dual          - iterations : 0         time: 0.00    
##     Simplex                 -                        time: 0.00    
##       Primal simplex        - iterations : 0         time: 0.00    
##       Dual simplex          - iterations : 0         time: 0.00    
##     Mixed integer           - relaxations: 0         time: 0.00</code></pre>
<p>We can print the various estimates.</p>
<pre class="r"><code>cat(sprintf(&quot;Objective value: %f\n&quot;, result$criterion))</code></pre>
<pre><code>## Objective value: 4.153289</code></pre>
<p>We only print the nonzero <span class="math inline">\(\beta\)</span> values.</p>
<pre class="r"><code>index &lt;- which(result$beta_hat != 0)
est.table &lt;- data.frame(matrix(result$beta_hat[index], nrow = 1))
names(est.table) &lt;- paste0(&quot;$\\beta_{&quot;, index, &quot;}$&quot;)
knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;%
    kable_styling(&quot;striped&quot;)</code></pre>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(\beta_{1}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\beta_{2}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\beta_{3}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\beta_{4}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\beta_{20}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.716
</td>
<td style="text-align:right;">
-1.385
</td>
<td style="text-align:right;">
2.217
</td>
<td style="text-align:right;">
0.285
</td>
<td style="text-align:right;">
-0.038
</td>
</tr>
</tbody>
</table>
<p>For this value of <span class="math inline">\(\lambda\)</span>, the nonzero <span class="math inline">\((\beta_1, \beta_2, \beta_3,\beta4)\)</span> are picked up along
with a few others <span class="math inline">\((\beta_{20}, \beta_{34},\beta_{39}).\)</span></p>
<p>The values for <span class="math inline">\(\theta_0\)</span>.</p>
<pre class="r"><code>est.table &lt;- data.frame(matrix(result$theta0_hat, nrow = 1))
names(est.table) &lt;- paste0(&quot;$\\theta_{0,&quot;, 1:K, &quot;}$&quot;)
knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;%
    kable_styling(&quot;striped&quot;)</code></pre>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(\theta_{0,1}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{0,2}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{0,3}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{0,4}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.136
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
-0.516
</td>
<td style="text-align:right;">
0.068
</td>
</tr>
</tbody>
</table>
<p>And just the first five rows of <span class="math inline">\(\Theta\)</span>, which happen to contain all
the nonzero values for this result.</p>
<pre class="r"><code>est.table &lt;- data.frame(result$theta_hat[1:5, ])
names(est.table) &lt;- paste0(&quot;$\\theta_{,&quot;, 1:K, &quot;}$&quot;)
knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;%
    kable_styling(&quot;striped&quot;)</code></pre>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(\theta_{,1}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{,2}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{,3}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\theta_{,4}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
0.528
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.085
</td>
<td style="text-align:right;">
0.308
</td>
</tr>
<tr>
<td style="text-align:right;">
0.063
</td>
<td style="text-align:right;">
-0.592
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
-0.185
</td>
</tr>
<tr>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
</tbody>
</table>
</div>
<div id="final-comments" class="section level2">
<h2>Final comments</h2>
<p>Typically, one would run the fits for various values of <span class="math inline">\(\lambda\)</span> and
choose one based on cross-validation and assess the prediction against
a test set. Here, even a single fit takes a while, but techniques
discussed in other articles here can be used to speed up the
computations.</p>
<p>A logistic regression using a pliable lasso model can be prototyped
similarly.</p>
</div>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.2 (2019-12-12)
## Platform: x86_64-apple-darwin19.2.0 (64-bit)
## Running under: macOS Catalina 10.15.2
## 
## Matrix products: default
## BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.7/lib/libopenblasp-r0.3.7.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
## [1] kableExtra_1.1.0 CVXR_1.0        
## 
## loaded via a namespace (and not attached):
##  [1] gmp_0.5-13.5      Rcpp_1.0.3        highr_0.8         compiler_3.6.2   
##  [5] pillar_1.4.3      tools_3.6.2       zeallot_0.1.0     digest_0.6.23    
##  [9] bit_1.1-14        viridisLite_0.3.0 lifecycle_0.1.0   evaluate_0.14    
## [13] tibble_2.1.3      lattice_0.20-38   pkgconfig_2.0.3   rlang_0.4.2      
## [17] Matrix_1.2-18     gurobi_9.0-0      rstudioapi_0.10   Rglpk_0.6-4      
## [21] yaml_2.2.0        blogdown_0.17     xfun_0.11         xml2_1.2.2       
## [25] httr_1.4.1        Rmpfr_0.7-2       stringr_1.4.0     knitr_1.26       
## [29] vctrs_0.2.1       hms_0.5.2         webshot_0.5.2     bit64_0.9-7      
## [33] grid_3.6.2        glue_1.3.1        R6_2.4.1          rmarkdown_2.0    
## [37] bookdown_0.16     readr_1.3.1       magrittr_1.5      rcbc_0.1.0.9001  
## [41] scales_1.1.0      backports_1.1.5   htmltools_0.4.0   assertthat_0.2.1 
## [45] rvest_0.3.5       colorspace_1.4-1  Rcplex_0.3-3      stringi_1.4.5    
## [49] Rmosek_9.1.0      munsell_0.5.0     slam_0.1-47       crayon_1.3.4</code></pre>
</div>
<div id="source" class="section level2">
<h2>Source</h2>
<p><a href="https://github.com/bnaras/cvxr_docs/blob/master/content/cvxr_examples/pliable-lasso.Rmd">R Markdown</a></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-tibsjhf:2017">
<p>Tibshirani, Robert J., and Jerome H. Friedman. 2017. “A Pliable Lasso.” <em>arXiv Preprint Arxiv:1712.00484</em>. <a href="https://arxiv.org/abs/1712.00484">https://arxiv.org/abs/1712.00484</a>.</p>
</div>
</div>
</div>
