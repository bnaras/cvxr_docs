---
title: Saturating Hinges Fit
author: Anqi Fu and Balasubramanian Narasimhan
date: '2017-11-02'
slug: cvxr_saturating_hinges
bibliography: ../bibtex/cvxr_refs.bib
link-citations: true
categories: []
tags: []
params:
  mode: ignore
  testdata_dir: test_data
  data_dir: saturating_hinges
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The following example comes from work on saturating splines in
<span class="citation">Boyd et al. (<a href="#ref-BoydHastie:2016" role="doc-biblioref">2016</a>)</span>. Adaptive regression splines are commonly used in
statistical modeling, but the instability they exhibit beyond their
boundary knots makes extrapolation dangerous. One way to correct this
issue for linear splines is to require they <em>saturate</em>: remain
constant outside their boundary. This problem can be solved using a
heuristic that is an extension of lasso regression, producing a
weighted sum of hinge functions, which we call a <em>saturating hinge</em>.</p>
<p>For simplicity, consider the univariate case with <span class="math inline">\(n = 1\)</span>. Assume we
are given knots <span class="math inline">\(t_1 &lt; t_2 &lt; \cdots &lt; t_k\)</span> where each
<span class="math inline">\(t_j \in {\mathbf R}\)</span>. Let <span class="math inline">\(h_j\)</span> be a hinge function at knot <span class="math inline">\(t_j\)</span>, ,
<span class="math inline">\(h_j(x) = \max(x-t_j,0)\)</span>, and define
<span class="math inline">\(f(x) = w_0 + \sum_{j=1}^k w_jh_j(x)\)</span>. We want to solve</p>
<p><span class="math display">\[
\begin{array}{ll} 
\underset{w_0,w}{\mbox{minimize}} &amp; \sum_{i=1}^m \ell(y_i, f(x_i)) + \lambda\|w\|_1 \\
\mbox{subject to} &amp; \sum_{j=1}^k w_j = 0
\end{array}
\]</span></p>
<p>for variables <span class="math inline">\((w_0,w) \in {\mathbf R} \times {\mathbf R}^k\)</span>. The function
<span class="math inline">\(\ell:{\mathbf R} \times {\mathbf R} \rightarrow {\mathbf R}\)</span> is the loss associated
with every observation, and <span class="math inline">\(\lambda \geq 0\)</span> is the penalty weight. In
choosing our knots, we set <span class="math inline">\(t_1 = \min(x_i)\)</span> and <span class="math inline">\(t_k = \max(x_i)\)</span> so
that by construction, the estimate <span class="math inline">\(\hat f\)</span> will be constant outside
<span class="math inline">\([t_1,t_k]\)</span>.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>We demonstrate this technique on the bone density data for female
patients from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-ESL" role="doc-biblioref">2001</a>)</span>, section 5.4. There are a total of <span class="math inline">\(m = 259\)</span>
observations. Our response <span class="math inline">\(y_i\)</span> is the change in spinal bone density
between two visits, and our predictor <span class="math inline">\(x_i\)</span> is the patient’s age. We
select <span class="math inline">\(k = 10\)</span> knots about evenly spaced across the range of <span class="math inline">\(X\)</span> and
fit a saturating hinge with squared error loss
<span class="math inline">\(\ell(y_i, f(x_i)) = (y_i - f(x_i))^2\)</span>.</p>
<pre class="r"><code>## Import and sort data
data(bone, package = &quot;ElemStatLearn&quot;)
X &lt;- bone[bone$gender == &quot;female&quot;,]$age
y &lt;- bone[bone$gender == &quot;female&quot;,]$spnbmd
ord &lt;- order(X, decreasing = FALSE)
X &lt;- X[ord]
y &lt;- y[ord]

## Choose knots evenly distributed along domain
k &lt;- 10
lambdas &lt;- c(1, 0.5, 0.01)
idx &lt;- floor(seq(1, length(X), length.out = k))
knots &lt;- X[idx]</code></pre>
<p>In <code>R</code>, we first define the estimation and loss functions:</p>
<pre class="r"><code>## Saturating hinge
f_est &lt;- function(x, knots, w0, w) {
    hinges &lt;- sapply(knots, function(t) { pmax(x - t, 0) })
    w0 + hinges %*% w
}

## Loss function
loss_obs &lt;- function(y, f) { (y - f)^2 }</code></pre>
<p>This allows us to easily test different losses and knot locations
later. The rest of the set-up is similar to previous examples. We
assume that <code>knots</code> is an <code>R</code> vector representing
<span class="math inline">\((t_1,\ldots,t_k)\)</span>.</p>
<pre class="r"><code>## Form problem
w0 &lt;- Variable(1)
w &lt;- Variable(k)
loss &lt;- sum(loss_obs(y, f_est(X, knots, w0, w)))
constr &lt;- list(sum(w) == 0)

xrange &lt;- seq(min(X), max(X), length.out = 100)
splines &lt;- matrix(0, nrow = length(xrange), ncol = length(lambdas))</code></pre>
<p>The optimal weights are retrieved using separate calls, as shown
below.</p>
<pre class="r"><code>for (i in seq_along(lambdas)) {
    lambda &lt;- lambdas[i]
    reg &lt;- lambda * p_norm(w, 1)
    obj &lt;- loss + reg
    prob &lt;- Problem(Minimize(obj), constr)

    ## Solve problem and save spline weights
    result &lt;- solve(prob)
    w0s &lt;- result$getValue(w0)
    ws &lt;- result$getValue(w)
    splines[, i] &lt;- f_est(xrange, knots, w0s, ws)
}</code></pre>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>We plot the fitted saturating hinges in Figure 1 below. As expected,
when <span class="math inline">\(\lambda\)</span> increases, the spline exhibits less variation and grows
flatter outside its boundaries.</p>
<pre class="r"><code>d &lt;- data.frame(xrange, splines)
names(d) &lt;- c(&quot;x&quot;, paste0(&quot;lambda_&quot;, seq_len(length(lambdas))))
plot.data &lt;- gather(d, key = &quot;lambda&quot;, value = &quot;spline&quot;,
                    &quot;lambda_1&quot;, &quot;lambda_2&quot;, &quot;lambda_3&quot;, factor_key = TRUE)
ggplot() +
    geom_point(mapping = aes(x = X, y = y)) +
    geom_line(data = plot.data, mapping = aes(x = x, y = spline, color = lambda)) +
    scale_color_discrete(name = expression(lambda),
                         labels = sprintf(&quot;%0.2f&quot;, lambdas)) +
    labs(x = &quot;Age&quot;, y = &quot;Change in Bone Density&quot;) +
    theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/cvxr_examples/saturating-hinges_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The squared error loss works well in this case, but the Huber loss is
preferred when the dataset contains large outliers. To see this, we
add 50 randomly generated
outliers to the bone density data and re-estimate the saturating
hinges.</p>
<pre class="r"><code>## Add outliers to data
set.seed(1)
nout &lt;- 50
X_out &lt;- runif(nout, min(X), max(X))
y_out &lt;- runif(nout, min(y), 3*max(y)) + 0.3
X_all &lt;- c(X, X_out)
y_all &lt;- c(y, y_out)

## Solve with squared error loss
loss_obs &lt;- function(y, f) { (y - f)^2 }
loss &lt;- sum(loss_obs(y_all, f_est(X_all, knots, w0, w)))
prob &lt;- Problem(Minimize(loss + reg), constr)
result &lt;- solve(prob)
spline_sq &lt;- f_est(xrange, knots, result$getValue(w0), result$getValue(w))

## Solve with Huber loss
loss_obs &lt;- function(y, f, M) { huber(y - f, M) }
loss &lt;- sum(loss_obs(y, f_est(X, knots, w0, w), 0.01))
prob &lt;- Problem(Minimize(loss + reg), constr)
result &lt;- solve(prob)
spline_hub &lt;- f_est(xrange, knots, result$getValue(w0), result$getValue(w))</code></pre>
<p>Figure 2 shows the results. For a Huber loss with <span class="math inline">\(M = 0.01\)</span>, the
resulting spline is fairly smooth and follows the shape of the
original data, as opposed to the spline using squared error loss,
which is biased upwards by a significant amount.</p>
<pre class="r"><code>d &lt;- data.frame(xrange, spline_hub, spline_sq)
names(d) &lt;- c(&quot;x&quot;, &quot;Huber&quot;, &quot;Squared&quot;)
plot.data &lt;- gather(d, key = &quot;loss&quot;, value = &quot;spline&quot;,
                    &quot;Huber&quot;, &quot;Squared&quot;, factor_key = TRUE)
ggplot() +
    geom_point(mapping = aes(x = X, y = y)) +
    geom_point(mapping = aes(x = X_out, y = y_out), color = &quot;orange&quot;) +
    geom_line(data = plot.data, mapping = aes(x = x, y = spline, color = loss)) +
    scale_color_discrete(name = &quot;Loss&quot;,
                         labels = c(&quot;Huber&quot;, &quot;Squared&quot;)) +
    labs(x = &quot;Age&quot;, y = &quot;Change in Bone Density&quot;) +
    theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/cvxr_examples/saturating-hinges_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.2 (2019-12-12)
## Platform: x86_64-apple-darwin19.2.0 (64-bit)
## Running under: macOS Catalina 10.15.2
## 
## Matrix products: default
## BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.7/lib/libopenblasp-r0.3.7.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
## [1] tidyr_1.0.0   ggplot2_3.2.1 CVXR_1.0     
## 
## loaded via a namespace (and not attached):
##  [1] gmp_0.5-13.5     Rcpp_1.0.3       compiler_3.6.2   pillar_1.4.3    
##  [5] tools_3.6.2      zeallot_0.1.0    digest_0.6.23    bit_1.1-14      
##  [9] evaluate_0.14    lifecycle_0.1.0  tibble_2.1.3     gtable_0.3.0    
## [13] lattice_0.20-38  pkgconfig_2.0.3  rlang_0.4.2      Matrix_1.2-18   
## [17] gurobi_9.0-0     Rglpk_0.6-4      yaml_2.2.0       blogdown_0.17   
## [21] xfun_0.11        osqp_0.6.0.3     withr_2.1.2      Rmpfr_0.7-2     
## [25] stringr_1.4.0    dplyr_0.8.3      knitr_1.26       vctrs_0.2.1     
## [29] tidyselect_0.2.5 bit64_0.9-7      grid_3.6.2       glue_1.3.1      
## [33] R6_2.4.1         rmarkdown_2.0    bookdown_0.16    farver_2.0.1    
## [37] purrr_0.3.3      magrittr_1.5     ellipsis_0.3.0   rcbc_0.1.0.9001 
## [41] backports_1.1.5  scales_1.1.0     htmltools_0.4.0  assertthat_0.2.1
## [45] colorspace_1.4-1 labeling_0.3     Rcplex_0.3-3     stringi_1.4.5   
## [49] Rmosek_9.1.0     lazyeval_0.2.2   munsell_0.5.0    slam_0.1-47     
## [53] crayon_1.3.4</code></pre>
</div>
<div id="source" class="section level2">
<h2>Source</h2>
<p><a href="https://github.com/bnaras/cvxr_docs/blob/master/content/cvxr_examples/saturating-hinges.Rmd">R Markdown</a></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-BoydHastie:2016">
<p>Boyd, N., T. Hastie, S. Boyd, B. Recht, and M. Jordan. 2016. “Saturating Splines and Feature Selection.” <em>arXiv Preprint arXiv:1609.06764</em>.</p>
</div>
<div id="ref-ESL">
<p>Hastie, T., R. Tibshirani, and J. Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
</div>
</div>
