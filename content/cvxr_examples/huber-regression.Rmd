---
title: Huber Regression
author: Anqi Fu and Balasubramanian Narasimhan
date: '2017-11-02'
slug: cvxr_huber-regression
bibliography: ../bibtex/cvxr_refs.bib
link-citations: true
categories: []
tags: []
---


```{r prereqs, message = FALSE, echo = FALSE}
library(CVXR)
library(ggplot2)
```
## Introduction

Huber regression [@Huber:1964] is a regression technique that
is robust to outliers. The idea is to use a different loss function
rather than the traditional least-squares; we solve

\begin{array}{ll} 
\underset{\beta}{\mbox{minimize}} & \sum_{i=1}^m \phi(y_i -
x_i^T\beta) 
\end{array}

for variable $\beta \in {\mathbf R}^n$, where the loss $\phi$ is the Huber
function with threshold $M > 0$,
\[
	\phi(u) = 
	\begin{cases}
		u^2 & \mbox{if } |u| \leq M \\
		2Mu - M^2 & \mbox{if } |u| > M.
	\end{cases}
\]

This function is identical to the least squares penalty for small
residuals, but on large residuals, its penalty is lower and increases
linearly rather than quadratically. It is thus more forgiving of
outliers.

## Example

We generate some problem data.

```{r}
n <- 1
m <- 450
M <- 1      ## Huber threshold
p <- 0.1    ## Fraction of responses with sign flipped

## Generate problem data
set.seed(1289)
beta_true <- 5 * matrix(stats::rnorm(n), nrow = n)
X <- matrix(stats::rnorm(m * n), nrow = m, ncol = n)
y_true <- X %*% beta_true
eps <- matrix(stats::rnorm(m), nrow = m)
```

We will randomly flip the sign of some responses to illustrate the
  robustness. 
 
```{r} 
factor <- 2*stats::rbinom(m, size = 1, prob = 1-p) - 1
y <- factor * y_true + eps
```

We can solve this problem both using ordinary least squares and huber
regression to compare.

```{r}
beta <- Variable(n)
rel_err <- norm(beta - beta_true, "F") / norm(beta_true, "F")

## OLS
obj <- sum((y - X %*% beta)^2)
prob <- Problem(Minimize(obj))
result <- solve(prob)
beta_ols <- result$getValue(beta)
err_ols <- result$getValue(rel_err)

## Solve Huber regression problem
obj <- sum(CVXR::huber(y - X %*% beta, M))
prob <- Problem(Minimize(obj))
result <- solve(prob)
beta_hub <- result$getValue(beta)
err_hub <- result$getValue(rel_err)
```

Finally, we also solve the OLS problem assuming we know the flipped
signs.

```{r}
## Solve ordinary least squares assuming sign flips known
obj <- sum((y - factor*(X %*% beta))^2)
prob <- Problem(Minimize(obj))
result <- solve(prob)
beta_prs <- result$getValue(beta)
err_prs <- result$getValue(rel_err)
```

We can now plot the fit against the measured responses. 

```{r}
d1 <- data.frame(X = X, y = y, sign = as.factor(factor))
d2 <- data.frame(X = rbind(X, X, X),
                 yHat = rbind(X %*% beta_ols,
                              X %*% beta_hub,
                              X %*% beta_prs),
                 Estimate = c(rep("OLS", m),
                              rep("Huber", m),
                              rep("Prescient", m)))
ggplot() +
    geom_point(data = d1, mapping = aes(x = X, y = y, color = sign)) +
    geom_line(data = d2, mapping = aes(x = X, y = yHat, color = Estimate))
```

As can be seen, the Huber line is closer to the prescient line.


## Session Info

```{r}
sessionInfo()
```

## Source

[R Markdown](https://github.com/bnaras/cvxr_docs/blob/master/content/cvxr_examples/huber-regression.Rmd)

## References
