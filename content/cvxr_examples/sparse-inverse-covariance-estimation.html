---
title: Sparse Inverse Covariance Estimation
author: Anqi Fu and Balasubramanian Narasimhan
date: '2017-11-02'
slug: cvxr_sparse_inverse_covariance_estimation
bibliography: ../bibtex/cvxr_refs.bib
link-citations: true
categories: []
tags: []
params:
  mode: ignore
  testdata_dir: test_data_1.0
  data_dir: sparse_inverse_covariance_estimation
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Assume we are given i.i.d. observations <span class="math inline">\(x_i \sim N(0,\Sigma)\)</span> for <span class="math inline">\(i
= 1,\ldots,m\)</span>, and the covariance matrix <span class="math inline">\(\Sigma \in {\mathbf S}_+^n\)</span>, the
set of symmetric positive semidefinite matrices, has a sparse inverse
<span class="math inline">\(S = \Sigma^{-1}\)</span>. Let <span class="math inline">\(Q = \frac{1}{m-1}\sum_{i=1}^m (x_i - \bar
x)(x_i - \bar x)^T\)</span> be our sample covariance. One way to estimate
<span class="math inline">\(\Sigma\)</span> is to maximize the log-likelihood with the prior knowledge
that <span class="math inline">\(S\)</span> is sparse <span class="citation">(<a href="#ref-spinvcov">Friedman, Hastie, and Tibshirani 2008</a>)</span>, which amounts to the optimization
problem:</p>
<p><span class="math display">\[
\begin{array}{ll} \underset{S}{\mbox{maximize}} &amp; \log\det(S) - \mbox{tr}(SQ) \\
\mbox{subject to} &amp; S \in {\mathbf S}_+^n, \quad \sum_{i=1}^n \sum_{j=1}^n |S_{ij}| \leq \alpha.
\end{array}
\]</span></p>
<p>The parameter <span class="math inline">\(\alpha \geq 0\)</span> controls the degree of sparsity. The problem
is convex, so we can solve it using <code>CVXR</code>.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>We’ll create a sparse positive semi-definite matrix <span class="math inline">\(S\)</span> using
synthetic data</p>
<pre class="r"><code>set.seed(1)
n &lt;- 10      ## Dimension of matrix
m &lt;- 1000    ## Number of samples

## Create sparse, symmetric PSD matrix S
A &lt;- rsparsematrix(n, n, 0.15, rand.x = stats::rnorm)
Strue &lt;- A %*% t(A) + 0.05 * diag(rep(1, n))    ## Force matrix to be strictly positive definite</code></pre>
<p>We can now create the covariance matrix <span class="math inline">\(R\)</span> as the inverse of <span class="math inline">\(S\)</span>.</p>
<pre class="r"><code>R &lt;- base::solve(Strue)</code></pre>
<p>As test data, we sample from a multivariate normal with the fact that
if <span class="math inline">\(Y \sim N(0, I)\)</span>, then <span class="math inline">\(R^{1/2}Y \sim N(0, R)\)</span> since <span class="math inline">\(R\)</span> is
symmetric.</p>
<pre class="r"><code>x_sample &lt;- matrix(stats::rnorm(n * m), nrow = m, ncol = n) %*% t(expm::sqrtm(R))
Q &lt;- cov(x_sample)    ## Sample covariance matrix</code></pre>
<p>Finally, we solve our convex program for a set of <span class="math inline">\(\alpha\)</span> values.</p>
<p><strong>Version 1.0 Note:</strong> Positive semi-definite variables are now
designated using <code>PSD = TRUE</code> rather than the <code>Semidef</code> function!</p>
<pre class="r"><code>alphas &lt;- c(10, 8, 6, 4, 1)
if (packageVersion(&quot;CVXR&quot;) &gt; &quot;0.99-7&quot;) {
    S  &lt;- Variable(n, n, PSD = TRUE)
} else {
    S &lt;- Semidef(n)    ## Variable constrained to positive semidefinite cone
}

obj &lt;- Maximize(log_det(S) - matrix_trace(S %*% Q))

S.est &lt;- lapply(alphas,
                function(alpha) {
                    constraints &lt;- list(sum(abs(S)) &lt;= alpha)
                    ## Form and solve optimization problem
                    prob &lt;- Problem(obj, constraints)
                    result &lt;- solve(prob)
                    
                    ## Create covariance matrix
                    R_hat &lt;- base::solve(result$getValue(S))
                    Sres &lt;- result$getValue(S)
                    Sres[abs(Sres) &lt;= 1e-4] &lt;- 0
                    Sres
                })</code></pre>
<p>In the code above, the <code>Semidef</code> constructor restricts <code>S</code> to the
positive semidefinite cone. In our objective, we use <code>CVXR</code> functions
for the log-determinant and trace. The expression <code>matrix_trace(S %*% Q)</code> is equivalent to `sum(diag(S %*% Q))}, but the former is preferred
because it is more efficient than making nested function calls.</p>
<p>However, a standalone atom does not exist for the determinant, so we
cannot replace <code>log_det(S)</code> with <code>log(det(S))</code> since <code>det</code> is
undefined for a <code>Semidef</code> object.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<p>The figures below depict the solutions for the above dataset with <span class="math inline">\(m =
1000, n = 10\)</span>, and <span class="math inline">\(S\)</span> containing 26% non-zero entries, represented by
the dark squares in the images below. The sparsity of our inverse
covariance estimate decreases for higher <span class="math inline">\(\alpha\)</span>, so that when
<span class="math inline">\(\alpha = 1\)</span>, most of the off-diagonal entries are zero, while if
<span class="math inline">\(\alpha = 10\)</span>, over half the matrix is dense. At <span class="math inline">\(\alpha = 4\)</span>, we
achieve the true percentage of non-zeros.</p>
<pre class="r"><code>do.call(multiplot, args = c(list(plotSpMat(Strue)),
                            mapply(plotSpMat, S.est, alphas, SIMPLIFY = FALSE),
                            list(layout = matrix(1:6, nrow = 2, byrow = TRUE))))</code></pre>
<p><img src="/cvxr_examples/sparse-inverse-covariance-estimation_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
</div>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.4.2 (2024-10-31)
## Platform: x86_64-apple-darwin20
## Running under: macOS Sequoia 15.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib 
## LAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## time zone: America/Los_Angeles
## tzcode source: internal
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices datasets  utils     methods  
## [8] base     
## 
## other attached packages:
## [1] expm_1.0-0    Matrix_1.7-1  ggplot2_3.5.1 CVXR_1.0-15  
## 
## loaded via a namespace (and not attached):
##  [1] gmp_0.7-5          clarabel_0.9.0.1   sass_0.4.9         utf8_1.2.4        
##  [5] generics_0.1.3     slam_0.1-54        blogdown_1.19      lattice_0.22-6    
##  [9] digest_0.6.37      magrittr_2.0.3     RColorBrewer_1.1-3 evaluate_1.0.1    
## [13] bookdown_0.41      fastmap_1.2.0      jsonlite_1.8.9     scs_3.2.4         
## [17] Rmosek_10.2.0      fansi_1.0.6        scales_1.3.0       codetools_0.2-20  
## [21] jquerylib_0.1.4    cli_3.6.3          Rmpfr_0.9-5        rlang_1.1.4       
## [25] Rglpk_0.6-5.1      bit64_4.5.2        munsell_0.5.1      withr_3.0.2       
## [29] cachem_1.1.0       yaml_2.3.10        tools_4.4.2        Rcplex_0.3-6      
## [33] rcbc_0.1.0.9001    dplyr_1.1.4        colorspace_2.1-1   gurobi_11.0-0     
## [37] assertthat_0.2.1   vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   
## [41] bit_4.5.0          pkgconfig_2.0.3    cccp_0.3-1         pillar_1.9.0      
## [45] bslib_0.8.0        gtable_0.3.6       glue_1.8.0         Rcpp_1.0.13-1     
## [49] highr_0.11         xfun_0.49          tibble_3.2.1       tidyselect_1.2.1  
## [53] knitr_1.48         farver_2.1.2       htmltools_0.5.8.1  labeling_0.4.3    
## [57] rmarkdown_2.29     compiler_4.4.2</code></pre>
</div>
<div id="source" class="section level2">
<h2>Source</h2>
<p><a href="https://github.com/bnaras/cvxr_docs/blob/master/content/cvxr_examples/sparse-inverse-covariance-estimation.Rmd">R Markdown</a></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-spinvcov" class="csl-entry">
Friedman, J., T. Hastie, and R. Tibshirani. 2008. <span>“Sparse Inverse Covariance Estimation with the Graphical Lasso.”</span> <em>Biostatistics</em> 9 (3): 432–41.
</div>
</div>
</div>
